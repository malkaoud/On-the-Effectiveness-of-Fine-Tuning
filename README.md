# On the Effectiveness of Limited-Data Large Language Model Fine-tuning for Arabic

This repository contains code and data for the paper titled "On the effectiveness of limited-data large language model fine-tuning for Arabic."

## Repository Structure

This repository is organized into three main directories:

-   `data/`: Contains the three benchmark datasets used for our experiments (ArSAS, ASND, and ArSarcasm).
-   `gemma/`: Contains code and data related to fine-tuning the open-source Gemma model (`gemma-3-27b-it`).
-   `openai_gpt4o_mini/`: Contains code and data related to fine-tuning the GPT-4o mini model via the OpenAI API.

## Citation

This work is currently under review for publication. If you use our code or findings, we would appreciate a citation to the paper once it is published. For now, please refer to this work by its title or this GitHub repository.



